apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: team-a
spec:
  # Template section that describes how to template out NodeClaim resources that Karpenter will provision
  # Karpenter will consider this template to be the minimum requirements needed to provision a Node using this NodePool
  # It will overlay this NodePool with Pods that need to schedule to further constrain the NodeClaims
  # Karpenter will provision to launch new Nodes for the cluster
  template:
    metadata:
      # Labels are arbitrary key-values that are applied to all nodes
      labels:
        billing-team: team-a

      # Annotations are arbitrary key-values that are applied to all nodes
      annotations:
        example.com/owner: "team-a"
    spec:
      # References the Cloud Provider's NodeClass resource, see your cloud provider specific documentation
      nodeClassRef:
        group: karpenter.k8s.aws # Updated since only a single version will be served
        kind: EC2NodeClass
        name: default

      # Note: changing this value in the nodepool will drift the nodeclaims.
      expireAfter: 720h | Never

      # Provisioned nodes will have these taints
      # Taints may prevent pods from scheduling if they are not tolerated by the pod.
      taints:
      - key: example.com/special-taint
        effect: NoSchedule
      # The amount of time that a node can be draining before it's forcibly deleted. A node begins draining when a delete call is made against it, starting
      # its finalization flow. Pods with TerminationGracePeriodSeconds will be deleted preemptively before this terminationGracePeriod ends to give as much time to cleanup as possible.
      # If your pod's terminationGracePeriodSeconds is larger than this terminationGracePeriod, Karpenter may forcibly delete the pod
      # before it has its full terminationGracePeriod to cleanup.

      # Note: changing this value in the nodepool will drift the nodeclaims.
      terminationGracePeriod: 48h

      # Requirements that constrain the parameters of provisioned nodes.
      # These requirements are combined with pod.spec.topologySpreadConstraints, pod.spec.affinity.nodeAffinity, pod.spec.affinity.podAffinity, and pod.spec.nodeSelector rules.
      # Operators { In, NotIn, Exists, DoesNotExist, Gt, and Lt } are supported.
      # https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#operators
      requirements:
      - key: "karpenter.k8s.aws/instance-category"
        operator: In
        values: [ "c5", "m5", "r5" ]
        # minValues here enforces the scheduler to consider at least that number of unique instance-category to schedule the pods.
        # This field is ALPHA and can be dropped or replaced at any time
        minValues: 2
      - key: "karpenter.k8s.aws/instance-family"
        operator: In
        values: [ "m5", "m5d", "c5", "c5d", "c4", "r4" ]
        minValues: 5
      - key: "karpenter.k8s.aws/instance-cpu"
        operator: In
        values: [ "4", "8", "16", "32" ]
      - key: "karpenter.k8s.aws/instance-hypervisor"
        operator: In
        values: [ "nitro" ]
      - key: "karpenter.k8s.aws/instance-generation"
        operator: Gt
        values: [ "2" ]
      - key: "topology.kubernetes.io/zone"
        operator: In
        values: [ "eu-west-1a", "eu-west-1b" ]
      - key: "kubernetes.io/arch"
        operator: In
        values: [ "arm64", "amd64" ]
      - key: "karpenter.sh/capacity-type"
        operator: In
        values: [ "spot", "on-demand" ]
  # Resource limits constrain the total size of the pool.
  # Limits prevent Karpenter from creating new instances once the limit is exceeded.
  limits:
    cpu: "100"
    memory: 1000Gi

  # Priority given to the NodePool when the scheduler considers which NodePool
  # to select. Higher weights indicate higher priority when comparing NodePools.
  # Specifying no weight is equivalent to specifying a weight of 0.
  weight: 10
